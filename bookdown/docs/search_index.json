[["index.html", "A Guide to an Image Processing Pipeline for Classification with Machine Learning Chapter 1 Introduction 1.1 About This Book 1.2 Introduction 1.3 How to Use This Book 1.4 Data Pipeline Overview", " A Guide to an Image Processing Pipeline for Classification with Machine Learning Henry Sun, Biniam Garomsa, Hector Ontiveros 2022-07-28 Chapter 1 Introduction 1.1 About This Book This book was authored to serve as a basic guide for using our data pipeline to process raw images using ROI software, VIA image annotation, and a random forest machine learning model. Special thanks goes to Audrey Thellman and Weston Slaughter for their guidance and mentorship. 1.2 Introduction The primary target users of this software are river ecologists looking to extract data from camera traps. Freshwater systems are losing ice rapidly due to rising global temperatures. Currently, studies on river ice ecology are patchy, and more so regarding small-scale rivers. Our team’s images are from the Hubbard Brook Experimental Forest in New Hampshire. Nine camera traps in as many watersheds have taken images daily for three years (see below for an example image) from which the Hubbard Brook Ecosystem Study and the U.S. Geological Survey can extract data using our product. 1.3 How to Use This Book The data pipeline referenced in this book was originally designed for use by scientists studying field camera images at Hubbard Brook Experimental Forest. However, our software can be viably used for classification with other types of field images. Each chapter will provide a broad overview with instructions on applying our data pipeline for generalized applications. Instructions for Hubbard Brook users (with images stored) in Google Drive will be kept separate from instructions for users with other types of images, as modifications to the scripts will likely be required when processing different images. More information about our pipeline and its functionality can be found in the documentation for each script, or on our GitHub repo here. 1.4 Data Pipeline Overview The data pipeline starts with raw images and finishes with a trained machine learning model which can classify pixels into groups of attributes. Each chapter of this book will cover one step in this pipeline. Renaming image files In this step, raw images have their file names converted to contain useful information including time-series data Region of interest To avoid interference from land/soil, select a polygonal region of interest containing the desired region VIA image annotation Using VGG image annotation software, classify pixels in masked images to serve as training data for the machine learning model Machine learning model Run the images through a trained model which will predict ice and snow cover "],["rename-raw-images.html", "Chapter 2 Rename Raw Images 2.1 Google Drive Files 2.2 Local Files", " Chapter 2 Rename Raw Images Renaming images is the first key step in this data processing pipeline. For our study, field camera traps in various watersheds at Hubbard Brook Experimental Forest took one photo each day over a time span of several years. The original file names were a non-descriptive series of numbers, following this step, they will contain information about the watershed the photo was taken at as well as time-series image metadata. These steps were designed to process files stored in a shared Google Drive by running the script in Google Colaboratory. Before renaming images contained in a local directory, a few modifications to the script will need to be made; however, the same general principles will still apply. For instructions on running the script on files on your local computer rather than in Google Drive, see Section 1.2. 2.1 Google Drive Files 2.1.1 Load Packages Before each session, first run the top 3 lines – these lines of code install the Tesseract Optical Character Recognition Engine, which allows us to later use the text_to_string function and read the timestamp from each image. Subsequently, load all required packages/libraries. apt install tesseract-ocr apt install libtesseract-dev pip install pytesseract import numpy as np import pandas as pd import re import os import shutil from google.colab import drive from glob import glob 2.1.2 Mount Google Drive # This will connect to your Google Drive. It will ask you to allow access drive.mount(&#39;/content/drive&#39;, force_remount=True) When using Google Colaboratory, before performing any file operations, you must mount your personal Google Drive. Find the code chunk with the above code in Colab and run it to allow access. Afterwards, make sure all file paths used in any functions are for your Google Drive specifically. To find a pathname, click the orange file icon on Google Colab’s sidebar, and then click content to navigate your Google Drive. Right-click and select copy path to copy the pathname (see below). 2.1.3 Copying Files This preliminary step is used when a backup or copy of the original data is needed. It will copy all files in the source directory not present in the target directory. This method uses shutil’s copytree function, which blanket copies all files within a specified directory. To handle issues caused by direct copying of files versus copying of subdirectories, these are copied separate from each other within the code. if missing_files == source_file_list: # Will copy entire source folder into destination when no subfolders/files are shared between the two shutil.copytree(source, destination + &#39;/&#39; + directory_name, ignore = shutil.ignore_patterns(&#39;*.gdoc&#39;, &#39;*.gsheet&#39;, &#39;*.gslides&#39;)) #1 else: for folder in missing_files: # Will copy all missing files/subfolders not present in the destination new_dst = destination + &#39;/&#39; + folder if os.path.isfile(folder) == False: # Copies all subfolders/subdirectories shutil.copytree(source + &#39;/&#39; + folder, new_dst, ignore = shutil.ignore_patterns(&#39;*.gdoc&#39;, &#39;*.gsheet&#39;, &#39;*.gslides&#39;)) #1 missing_files.remove(folder) else: # Copies files not contained within a subdirectory shutil.copy(source + &#39;/&#39; + folder, destination) missing_files.remove(folder) print(&quot;These folders/files were not copied (ignore if list is empty): &quot;) print(missing_files) Copying any Google files, be it Google Docs, Slides, Sheets, Drawings, etc. must be done manually as these files are special and not able to be copied using shutil1. For any additional file extensions to avoid copying, specify them as arguments for shutil.ignore_patterns. 2.1.3.1 Main Method Finally, to copy the files, simply call copy_files within the main method. This method takes 3 arguments – the file path for the source folder, the file path for the destination folder, and a folder name for the folder created if the source and destination directories share no files. args = (&quot;/content/drive/MyDrive/Duke 2022-2023/Data+/2_Camera Trap photos/Stream Photos/On_Deck&quot;, &quot;/content/drive/MyDrive/Duke 2022-2023/Data+/2_Camera Trap photos/COPY of data for script/On_Deck&quot;, &quot;Newly_uploaded_data&quot;) copy_files(*args) 2.1.4 Renaming Files The renaming files script takes advantage of the Tesseract OCR Engine to read the time stamp on the image. This string is then parsed to generate time series information. There are many complementary methods in this script; for more information, see documentation within the script itself. Be sure to allocate time for the script to run, especially on folders containing large amounts of image files2 The pixel parameters within extract_timeStamp are designed for the images taken by the Bushnell field cameras used in our study. They may need to be edited manually for other classes of images. Pixel coordinates are identified with a pair of integers [x,y] where x is the column number and y is the row number. On a pixel grid, the origin is at the top left of the image. Pixel row values go from top to bottom, whereas pixel column values go from left to right. def extract_timeStamp(pic_address): &#39;&#39;&#39; Extract time Stamp from picture file. From the bottom right of each picture file, the time stamp is read as image using cv2. It is then converted to a string. text which is then checked for format and subsequently returned through match_date_format. Parameters ---------- pic_address : full source address of current picture file. Returns ------- match_date_format.group(0) : unaltered timeStamp from bottom of the photo as a string. &#39;&#39;&#39; # print(pic_address) img = cv2.imread(pic_address) #read as an image # check if the timestamp is the correct format date_pattern = &quot;\\d\\d-\\d\\d-\\d\\d\\d\\d \\d\\d:\\d\\d:\\d\\d&quot; # eg 12-12-2020 11:59:32 loop = 1 size_extension=0 x,y,z = np.shape(img) x = (x//1000)*1000 y = (y//1000)*1000 # print(x,y,z) while loop&gt;0: ts = img[2352 - size_extension:, 2000-size_extension:, :] #(change if sizing conventions change!) text = pytesseract.image_to_string(ts) match_date_format = re.search(date_pattern,text) if match_date_format: # found timestamp, return break ts_2 = img[x - size_extension:, x-size_extension:, :] #(change if sizing conventions change!) text_2= pytesseract.image_to_string(ts_2) match_date_format = re.search(date_pattern,text_2) if match_date_format: # found timestamp, return break size_extension+=100 loop-=1 if loop ==0: # reached end of loop without finding correct timestamp print(&quot;Correct timestamp not found&quot;) else: return match_date_format.group(0) As with before, make sure Google Drive is mounted, and all relevant packages/libraries are imported. Then, update the file paths3 and run the main method (below). from glob import glob #collect all folder paths from newly uploaded data on folder folder_list = glob(&quot;/content/drive/MyDrive/2_Camera Trap photos/COPY of data for script/Newly_uploaded_data/*/&quot;, recursive = True) # collect all folder path from on deck folder folder_list.extend(glob(&quot;/content/drive/MyDrive/2_Camera Trap photos/COPY of data for script/On_Deck/*/&quot;, recursive = True)) # extract folder_name folder_list = [f[:-1] for f in folder_list] i = 0 file_df = pd.read_csv(&quot;/content/drive/MyDrive/2_Camera Trap photos/COPY of data for script/Testing destination/file_df.csv&quot;) # for each folder rename and add them to the new destination - dst for folder in folder_list: print(i,&quot;/&quot;, len(folder_list)) i+=1 # destination to save labeled images dst = &quot;/content/drive/MyDrive/2_Camera Trap photos/project_dir/labeled_image_files&quot; save_as_zip = False #will unzip if necessary folder, unzipped = unzip_src(folder) # #create new destination folder fdr_name, fdr_dst = new_folder(folder, dst) if os.path.exists(fdr_dst): print(&quot;path already exists&quot;) else: print(&quot;new path&quot;) os.mkdir(fdr_dst) print(folder) print(fdr_name) print(fdr_dst) rename_images(folder, fdr_name, fdr_dst, file_df = file_df) The script generates a pandas dataframe which contains the old filename, new filename, folder name containing the image, as well as the image status (whether or not it was renamed successfully). This dataframe is then exported to a .csv file in a user-specified destination. See below for an example. file_df = pd.DataFrame(file_names_list, columns = [&quot;old_name&quot;, &quot;new_name&quot;,&quot;status&quot;, &quot;note&quot;,&quot;old_folder&quot;]) file_df.to_csv(dst+&quot;/&quot;+&quot;file_df.csv&quot;) file_df.head() 2.1.4.1 Manual Renames While this script works for the vast majority of images, occasionally some images will fail to rename correctly. For any files where the timestamp generated a file name which does not match a valid date, run the below chunk of code to identify them. # Load in created csv, returns file name and path if extracted timestamp is not in range file_df = pd.read_csv(&quot;/content/drive/MyDrive/2_Camera Trap photos/project_dir/labeled_image_files/file_df.csv&quot;) file_df = file_df[file_df[&quot;new_name&quot;].notnull()] for index, row in file_df.iterrows(): #Check month range if int(row[&quot;new_name&quot;][13:15]) &gt; 12 or int(row[&quot;new_name&quot;][13:15]) &lt; 0: print(row[&quot;new_name&quot;]+&quot; Month not in range, check name in folder :&quot;+row[&quot;old_folder&quot;]) #Check year range if int(row[&quot;new_name&quot;][9:13]) &gt; 2022 or int(row[&quot;new_name&quot;][9:13]) &lt; 2018: print(row[&quot;new_name&quot;]+&quot; Year not in range, check name in folder :&quot;+row[&quot;old_folder&quot;]) #Check day range if int(row[&quot;new_name&quot;][15:17]) &gt; 31 or int(row[&quot;new_name&quot;][15:17]) &lt; 0: print(row[&quot;new_name&quot;]+&quot; Day not in range, check name in folder :&quot;+row[&quot;old_folder&quot;]) Another common occurrence is when the timestamp failed to generate altogether due to pytesseract.image_to_string failing. This is sometimes unavoidable and requires manual renaming of the file. However, this should not be a frequent occurrence due to the built-in loop in extract_timeStamp. 2.2 Local Files This section covers how to run the copy_files method and rename_script script on files contained in a local directory. This is especially relevant for users aiming to rename images that are not from Hubbard Brook experimental forest, and/or do not meet the specifications of our script (i.e the timestamp is located at a different position on the image, or ) 2.2.1 Load Packages When using files on your local computer, first install pytesseract - documentation and more information on how to do this is available here. Then, load all packages as needed. import pytesseract import numpy as np import pandas as pd import re import os import shutil from glob import glob 2.2.2 Copying Files The shutil and os libraries can be used in the same manner on a local machine as in Google Drive. Therefore, no modifications are required to run the copy_files method on local files on your computer. Refer to the instructions above in section 1.1.3. Ensure the path arguments in the main method map to directories stored in your local machine. 2.2.3 Renaming Files The script for renaming files is designed with Hubbard Brook images in mind. The format for an image’s new name is Hbwtr_watershed number_date_time.JPG. The date and time elements are extracted from the image, whereas the watershed number is part of the name of the source directory. If you would like to follow a different naming convention, the below method must be changed (generate_picName) to reflect that. For example, changing new_name to \"Watershed\"ws_num +'_' + date + .'jpg'\" would output Watershed9_12042020.jpg for an image taken on December 4th, 2020 at Watershed no. 9. def generate_picName(fdr_name, tStamp): ws_num = fdr_name[1] #!!!this should be changed if src_elements[-2][1] will not be watershed number!! stamp_elements = re.split(&#39;[\\n: -]&#39;, tStamp) date = stamp_elements[2] + stamp_elements[0] + stamp_elements[1] time = stamp_elements[3] + stamp_elements[4] + stamp_elements[5] new_name = &quot;Hbwtr_w&quot; + ws_num + &#39;_&#39; + date + &#39;_&#39; + time + &#39;.JPG&#39; return new_name 2.2.3.1 Changing Timestamp The most complicated and error-prone step in the renaming process is extracting a timestamp. The way it is done in the current code is by starting with the pixel location of the timestamp, attempting to read the timestamp with image_to_string, and zooming out if the timestamp loop = 1 size_extension=0 x,y,z = np.shape(img) x = (x//1000)*1000 y = (y//1000)*1000 # print(x,y,z) while loop&gt;0: ts = img[2352 - size_extension:, 2000-size_extension:, :] #(change if sizing conventions change!) text = pytesseract.image_to_string(ts) match_date_format = re.search(date_pattern,text) if match_date_format: # found timestamp, return break ts_2 = img[x - size_extension:, x-size_extension:, :] #(change if sizing conventions change!) text_2= pytesseract.image_to_string(ts_2) match_date_format = re.search(date_pattern,text_2) if match_date_format: # found timestamp, return break size_extension+=100 loop-=1 if loop ==0: # reached end of loop without finding correct timestamp print(&quot;Correct timestamp not found&quot;) Our script is only able to extract the timestamp if it is present within the image. If the image you use does not contain any time-series data, we recommend removing the extract_timeStamp method and changing the file naming format to reflect that. After these changes are made, the script should again function identically to the one housed in Google Colab. Update the file path and run the main method to rename your files. The script will occasionally throw some unavoidable errors, such as the image_to_string method failing to extract a timestamp. To deal with these fringe cases, see section 1.1.4.1 (Manual Renames) above. For more information, see the comments within the script and text blocks in the Jupyter Notebook.↩︎ If you are looking to reduce the script’s runtime, one method is to remove the loop within extract_timeStamp which searches multiple times for the correct timestamp if it is not found initially. However, this will increase the number of files which failed to be renamed correctly.↩︎ It may be a good idea to remove or make a note of any non-image files within the folder, as these will throw errors.↩︎ "],["selecting-region-of-interest.html", "Chapter 3 Selecting Region of Interest 3.1 Using the Script 3.2 RoiPoly Functionalities 3.3 Documentation", " Chapter 3 Selecting Region of Interest This chapter provides an overview about the Python scripts used to create a polygonal region of interest and mask for images contained in folders. A region of interest is often needed to eliminate irrelevant regions or sections of the image. In our case, we wanted to focus solely on stream water/ice rather than surrounding rocks or shrubbery (these could interfere with the machine learning step later). Before beginning this section, ensure that all image files are properly named. interactive_ROI_app.py is the script used to create a region of interest and mask for images within a folder. Below is a visual aid concept map that outlines the steps within this process. 3.1 Using the Script This section will explain the necessary elements and steps for you to follow while using this script. For more information about how the script works, see Section 3.2; for more information about the functions within the script, see Section 3.3. 3.1.1 Import Packages Before running the script, load in all necessary packages. While testing our script, we have found that the “Qt5Agg” backend works best for Windows system while the “MacOSX” backend works best for Apple. These backends allow us to work interactively with the python plotting library matplotlib. For more information, visit matplotlib’s official website.4 During testing of the script, we found PyCharm to be the best IDE for running the script, because vscode didn’t support interactivity through matplotlib. Thus, we recommend using PyCharm to run this program. import re import matplotlib as mpl import os.path import pandas as pd from PIL import Image mpl.use(&#39;Qt5Agg&#39;) # backend import cv2 from roipoly import RoiPoly import glob2 import numpy as np import matplotlib.pyplot as plt from matplotlib.widgets import Button from collections import OrderedDict from matplotlib.path import Path as MplPath 3.1.2 Set Folder Path Our script denotes the file path to a folder and stores it in a variable folder_path. Change the example path to the path of the folder for which to operate on. We then use the .glob function of the glob package to store the file path for each image into the variable image_folder. folder_path = r&quot;\\Example\\Path\\To\\Folder&quot; image_folder = glob2.glob(folder_path + &quot;/*&quot;) 3.1.3 Drawing ROIs After setting the folder path, run the script to open a popup window – this is the matplotlib interactive interface. It should display the first image in image_folder and give you the option to select an ROI. In the matplotlib interactive interface, a point is drawn by left-clicking. By left-clicking again, a new point is created and the line becomes static. A new line is again shown from the last point to the user cursor. To complete a figure, the user right- or double-clicks, bounding the last selected point to the first. The polygon created within the image is the region of interest (ROI). Once you have finished drawing your ROI, click the Confirm button to apply the ROI and mask to all remaining images in the folder. After the mask has been applied, you can view the mask overlaid on all the remaining images (or prior images) by clicking the Previous or Next buttons. In some cases, the original ROI may no longer fit the new image - this is often the case if, for instance, snow melts and the water levels of the river rise. In such cases, redrawing the ROI may be necessary. To do this, click the Restart masking button, which should again bring up the interactive interface and allow you to redraw and confirm an ROI. Once you are satisfied with your new ROI, click the Confirm button to apply the new mask to the remainder of the images in the folder, and keep scrolling with the Previous and Next buttons. Then, click Finish masking when you are done. This will close the ROI window and generate data frames containing information about the masked images, as well as a folder containing the masked images. 3.1.4 Output After you are done drawing ROIs, the script will generate three seperate outputs automatically. See the photos below for examples. The first is a wateryear folder containing masked images. The second is a dataframe that stores each mask_id to its associated mask. Finally, the script will generate a dataframe that stores each mask_id to its associated date. 3.2 RoiPoly Functionalities RoiPoly5 is the python module from which our mouse click events are handled. The functions within this script allow the user to create an ROI by drawing a polygon with mouse clicks. The following section will provide a descriptive overview of the script and its functions. 3.2.1 ImageFile Class The script contains a class named ImageFile. Objects in this class have information from a file along a specific filepath as their attributes. class ImageFile: &quot;&quot;&quot; Image File class to save file path, file name, date, mask_id&quot;&quot;&quot; def __init__(self, filename): self.path = filename self.image_name = filename.split(&quot;\\\\&quot;)[-1] self.date = self.get_date() self.mm, self.dd, self.yy = self.date.split(&quot;/&quot;) self.mask_id = None There is an ImageFile object for each image in the folder image_folder. Within this class are functions that extract the time stamp and an array of RGB values for each image. Additionally, the water year for each image is derived from its timestamp, as is a sliced-array version of each image for faster plotting. These objects are appended to image_file_list, which is then sorted by water years. A list with complete information of every file in image_folder is appended to image_file_list, which is then sorted and converted to a list of arrays for plotting purposes. image_file_list = [] for filename in image_folder: filetype = filename[-4:] # Check if the file name ends with &quot;.JPG&quot; or &quot;.jpg&quot; if filetype.lower() != &quot;.jpg&quot;: continue curr_IF = ImageFile(filename) image_file_list.append(curr_IF) # sort by year, then month, then day image_file_list = np.array(sorted(image_file_list, key=lambda x: (x.yy, x.mm, x.dd))) 3.2.2 First ROI and Masking Function Upon running the script, a popup window will open displaying the first image in image_folder. The user can then select a polygonal ROI and apply a mask. Each selected point in a created polygon is stored into poly_verts (short for polygon vertices), which is used to create the mask outline for the region of interest. def get_mask_poly_verts(image, poly_verts, on_original=False): if len(np.shape(image)) == 3: ny, nx, nz = np.shape(image) else: ny, nx = np.shape(image) # if mask is applied to original, each coordinate is multiplied by 2 if on_original: poly_verts = [(2 * x, 2 * y) for (x, y) in poly_verts] x, y = np.meshgrid(np.arange(nx), np.arange(ny)) x, y = x.flatten(), y.flatten() points = np.vstack((x, y)).T roi_path = MplPath(poly_verts) mask = roi_path.contains_points(points).reshape((ny, nx)) return mask After creating a region of interest, the user must click the Confirm button to proceed and apply the mask. These buttons are part of matplotlib’s Button module. Creating one requires an event function, as well as button initialization as seen below for the Confirm button. The confirm_roi event is triggered when the button is clicked. confirm_ax = plt.axes([0.81, 0.05, 0.1, 0.075]) confirm_button = Button(confirm_ax, &#39;Confirm&#39;) confirm_button.on_clicked(confirm_roi) confirm_button._button = confirm_button def confirm_roi(event): &quot;&quot;&quot; Callback event for confirm button If users select ROI and hit confirm, save the poly_verts and apply it to the rest of images Then, start showing next and previous buttons &quot;&quot;&quot; # save current mask&#39;s poly_verts starting from start_img_ind index for i in range(start_img_ind, len(image_file_list)): poly_verts_list[i] = curr_poly_verts img_display_axis.set_title(&quot;Choose next or redraw ROI for {}&quot;.format(image_file_list[start_img_ind].date)) # button to show next and prev masked images _ = show_next_prev() We use Boolean algebra to apply the mask onto the image, rendering everything outside of the ROI black. After the ROI is confirmed and mask is applied, users can click the Previous and Next buttons to view subsequent/prior images in the folder with the mask applied. These are part of the callback function Callback, which makes sliding through a folder possible through indexing. By indexing, each image is drawn with its associated date in the figure title. def next(self, event): &quot;&quot;&quot; :param event: event callback for matplotlib button Slide to the next image in folder and display it &quot;&quot;&quot; self.index += 1 if not self.index_in_range(): print(&quot;Reached End of Folder&quot;) self.index -= 1 return im = self.get_masked_img() img_display.set_data(im) img_display_axis.set_title(&quot;Click next or draw new ROI for Date: {}&quot;.format(image_file_list[self.index].get_date())) plt.draw() def prev(self, event): &quot;&quot;&quot; :param event: event callback for matplotlib button Slide to the previous image in folder and display it &quot;&quot;&quot; self.index -= 1 if not self.index_in_range(): print(&quot;Reached Start of Folder&quot;) self.index += 1 return im = self.get_masked_img() img_display.set_data(im) img_display_axis.set_title(&quot;Click next or draw new ROI for Date: {}&quot;.format(image_file_list[self.index].get_date())) plt.draw() To redraw the ROI and apply it to the remaining images, the user must click the Restart masking button. The user can then create a new region of interest and click Confirm to proceed to apply the new mask. After creating a new ROI, the user has the option to either confirm or to redraw the ROI again. The underlying dynamics of the restart_masking button can be seen here. def restart_masking(event): &quot;&quot;&quot; :param event: Callback event when user restarts masking Clears plot and begin a new ROI masking session &quot;&quot;&quot; global my_roi, confirm_button, restart_masking_button, img_display, img_display_axis, start_img_ind, curr_mask, curr_poly_verts # clear plot plt.clf() # create new plot fg_2 = plt.gcf() fg_2.subplots_adjust(left=0.3, bottom=0.25) fg_2.set_size_inches(w, h, forward=True) # change the content of image on curr axis img_display_axis = plt.gca() if not callback.index_in_range(): print(&quot;OUT OF RANGE&quot;) return curr_ind = callback.index curr_obj = image_file_list[curr_ind] img_display_axis.set_title(&quot;Confirm ROI? Date: {}&quot;.format(curr_obj.get_date())) img_display = img_display_axis.imshow(curr_obj.read_img_sliced()) # display new ROI pop up my_roi = RoiPoly(color=&#39;r&#39;, close_fig=False) # wait until the user finishes selecting ROI while not my_roi.finished_clicking: plt.pause(0.01) # mask current image and display cp = curr_obj.read_img_sliced().copy() curr_mask, curr_poly_verts = my_roi.get_mask(cp) cp = apply_mask(cp, curr_mask) start_img_ind = curr_ind img_display = img_display_axis.imshow(cp) # Create a confirm mask button for new session confirm_ax = plt.axes([0.81, 0.05, 0.1, 0.075]) confirm_button = Button(confirm_ax, &#39;Confirm&#39;) confirm_button.on_clicked(confirm_roi) confirm_button._button = confirm_button # Create a restart mask button for new session restart_masking_ax = plt.axes([0.1, 0.05, 0.3, 0.075]) restart_masking_button = Button(restart_masking_ax, &quot;Restart masking&quot;) restart_masking_button.on_clicked(restart_masking) plt.draw() Once masking has been complete, click the Finish masking button to close the figure. This button event also generates the output of the script. This event is slow and hefty, expect a long processing time. def finish_masking(event): &quot;&quot;&quot; :param event: Callback event for finish masking button save dataframe linking mask_id to actual mask (mask_df) create water year folders save dataframe linking date to mask_id (date_mask_df) apply masks on original images and save them in their respective water year folders close plot &quot;&quot;&quot; global poly_verts_list # collect unique poly_verts and assign mask_ids to them poly_verts_unique_list = [] mask_id = -1 for i in range(len(poly_verts_list)): if mask_id == -1 or poly_verts_list[i - 1] != poly_verts_list[i]: mask_id += 1 poly_verts_unique_list.append(poly_verts_list[i]) # assign mask_ids to all ImageFile objects image_file_list[i].mask_id = mask_id # Save a mask_df data frame with columns mask_id-&gt; actual mask(poly_verts) and save it mask_df = pd.DataFrame(list(zip(poly_verts_unique_list)), columns=[&quot;poly_verts&quot;]) mask_df.index.name = &quot;mask_id&quot; mask_df_dst = folder_path + &quot;/&quot; + &quot;mask_df.csv&quot; mask_df.to_csv(mask_df_dst) # print(mask_df.head()) # collect all information from ImageFile Objects image_file_info = pd.DataFrame( [(i.date, i.mask_id, i.path, i.get_water_year(), ind, poly_verts_list[ind]) for ind, i in enumerate(image_file_list)], columns=[&quot;Date&quot;, &quot;mask_id&quot;, &quot;file_path&quot;, &quot;WY&quot;, &quot;list_index&quot;, &quot;poly_verts&quot;]) image_file_info.set_index(&quot;WY&quot;, inplace=True) # print(image_file_info.head()) # list of water years list_wy = list(image_file_info.index.unique()) print(&quot;STARTED SAVING&quot;) print(&quot;This takes about 1 second per an image file&quot;) for water_year in list_wy: # Create folders for each water year wy_dest = folder_path + &quot;/&quot; + &quot;WY&quot; + str(water_year) if not os.path.exists(wy_dest): os.mkdir(wy_dest) # loop through index of image_file_objects and save original images with their mask df = image_file_info[image_file_info.index == water_year] # save a date_mask dataframe with columns date-&gt; mask_id -&gt; file name date_mask_df = df.reset_index()[[&quot;Date&quot;, &quot;mask_id&quot;]].set_index(&quot;Date&quot;) date_mask_df.to_csv(wy_dest + &quot;/&quot; + &quot;date_mask.csv&quot;) # mask images within a selected water_year for index, row in df.iterrows(): folder_index = row[&quot;list_index&quot;] curr_file_path = row[&quot;file_path&quot;] # save masked image to WY destination curr_obj = image_file_list[folder_index] curr_file_name = curr_obj.image_name curr_original_image = curr_obj.read_img_orig().copy() curr_original_mask = get_mask_poly_verts(curr_original_image, poly_verts_list[folder_index], on_original=True) curr_original_image = apply_mask(curr_original_image, curr_original_mask) curr_img_save_dest = wy_dest + &quot;/&quot; + curr_file_name # save curr_original_image Image.fromarray(np.array(curr_original_image)).save(curr_img_save_dest) print(&quot;FINISHED SAVING&quot;) plt.close() 3.3 Documentation For complete documentation and explanations behind the code, see the script itself. Below is a list of all functions within the script and a brief description of their purpose. Function Description init Constructor get_date Extracts date pattern (MM/DD/YY) from file name (eg. Hbwtr_w3_20200315_115918.JPG) get_water_year Extracts water year from dates (a water year runs from October 1st of the year prior to September 30th of the current year) read_img_orig Reads image path and returns original image (as np.array) read_img_sliced Reads image path and returns sliced image (np.array) for faster display next Slides to next image in image folder and displays it prev Slides to the previous image in folder and displays it index_in_range Checks if the current index is within the range of the image_file_list get_masked_img Apply mask from poly_verts_list and return masked image start_roi_selection Allows user to select and confirm ROI show_first_image Displays the first image confirm_ROI If ROI is confirmed, save poly_verts and apply to the remaining images show_next_prev Creates next, previous, and finish buttons restart_masking Clears plot and begin a new ROI masking session finish_masking Saves dataframe linking mask_id to actual mask (mask_df), creates water year folders, saves dataframe linking date to mask_id (date_mask_df), applies masks on original images and saves them in their respective water year folders, then closes plot apply_mask Applies mask to image get_mask_poly_verts Returns coordinates for image mask that can be applied to image Information on matplotlib backends can be found at https://matplotlib.org/stable/users/explain/backends.html↩︎ Our version of RoiPoly is derived from jdoepfert’s roipoly.py, whose module can be found on at https://github.com/jdoepfert/roipoly.py↩︎ "],["classifying-image-attributes-with-via.html", "Chapter 4 Classifying Image Attributes with VIA 4.1 Creating Project 4.2 Creating Attributes 4.3 Classify Pixels 4.4 For Hubbard Brook Users", " Chapter 4 Classifying Image Attributes with VIA After completing image masking, the next step of image classification is done entirely in VGG Image Annotator (VIA). This is an HTML software found online, and the demo can be accessed here. It should look like this: In our data pipeline, fully classified images become the training data for our machine learning model. Thus, it is important to have well-annotated images before proceeding onto the final step. 4.1 Creating Project If a project already exists in VIA, navigate to the project tab in the menu bar and click load project to load in a preexisting .json file. Otherwise, remove the demo images and upload your own images using add files. At any point during annotation, these files may be saved into a .json project using Project &gt; Save from the menu bar. 4.2 Creating Attributes The next step is to create ‘attributes,’ which are effectively characteristics for pixels in the image. To create attributes for pixel classification, navigate to the bottom left-hand side of the screen to name and assign characteristics for the attribute. More information about declaring attributes as well as answers to other questions you may have about using VIA can be found on the user guide, available here. 4.3 Classify Pixels Classify pixels by dragging and clicking on the image to create boxes or shapes using the desired region shape, and then assign them to attributes. Repeat this process for all images in the folder. See below for an example using VIA’s demo file. If you must stop midway, save your project by clicking on the Settings pane in the menu bar and following the instructions. This will allow you to load in your previously saved progress directly in the future, rather than starting from the demo. When finished, export your annotations as a .csv and .json file for use in the machine learning model. Here is an example CSV-file output, containing information about the file and region attributes. 4.4 For Hubbard Brook Users In our project, the classified images outputted from VIA are the training data for the machine learning model to classify snow, ice, and leaf cover. Our project used 12 different attributes, including ice_t (transparent ice), leaf_green, leaf_fall, open_water, and snow_o (opaque snow), among others. To load the attributes that we used in our project, clicking this link will open a text file. Copy the contents of this file and save it as a .json file. Then, select load project from the menu bar, and upload the .json file. This loads a sample image and all 12 attribute classifications. Ideally, for each image, obtain at least 20 labels. "],["random-forest-image-classification.html", "Chapter 5 Random Forest Image Classification 5.1 Introduction to Random Forest 5.2 Using the Script 5.3 Future Improvements", " Chapter 5 Random Forest Image Classification The last step in the data pipeline is to run a random forest model to classify images. The script for running the machine learning model is ml_model_final.ipynb (hosted in Jupyter Notebook). 5.1 Introduction to Random Forest Our machine learning model used a random forest model, which is a type of ensemble learning method for classification. It works by taking a set number of decision trees all operating independently on a random subset of the data, and outputting the result selected by the largest number of trees. We chose a RF model for machine learning because it typically does a better job of not overfitting. This was important for our study at Hubbard Brook, because our images contained a large class imbalance. See the image below for an example class distribution (opaque snow and dark open water were far more prevalent than riffles/green leaves at this location). Other benefits of the random forest model are that it easily provides variable/feature importance, which is useful when evaluating the model’s performance. Additionally, we have tried an unsupervised learning approach with a convolutional neural network (CNN), which did not perform well on images with both leaves and ice. 5.2 Using the Script 5.2.1 Data Preparation First, load in all necessary packages. import pandas as pd import matplotlib.pyplot as plt import numpy as np import seaborn as sns from sklearn.model_selection import train_test_split, RandomizedSearchCV,GridSearchCV from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import classification_report,confusion_matrix, accuracy_score from sklearn.metrics import ConfusionMatrixDisplay import os %matplotlib inline from PIL import Image import warnings warnings.filterwarnings(&quot;ignore&quot;) The first step is to wrangle the data so it can be fed into the model. Read in your .csv files and run all the code chunks The below code chunk contains the function wrangle_data for data wrangling which create data and temperature columns for the dataframe. Additionally, the function pick_samples tries to combat our class imbalances by creating a max class size and then returning a sampled dataframe. def wrangle_data(df): &#39;&#39;&#39; Create date and temperature columns for dataframes &#39;&#39;&#39; # remove duplicate RGB df = df.drop_duplicates() df[&quot;date&quot;] = pd.to_datetime(df[&quot;date&quot;]) df[&quot;temperature&quot;] = df.temperature.apply(lambda a: float(a[:-1]) if type(a) != float else np.nan) return df def pick_samples(df): &#39;&#39;&#39; pick samples from each class with defined max class size &#39;&#39;&#39; sampled_df = df.head(1) max_class_size = 100 for date in df.orig_name.unique(): date_grouped = df[df.orig_name==date] nth = len(date_grouped)//max_class_size if nth==0: sampled_df = pd.concat([sampled_df,date_grouped]) else: curr_group = date_grouped.iloc[::nth,:] sampled_df = pd.concat([sampled_df,curr_group]) sampled_df.drop_duplicates(inplace=True) return sampled_df After the data wrangling is complete, the notebook first runs the model - a low accuracy will be corrected through the next step of hyperparameter tuning. 5.2.2 Hyperparameter Tuning Hyperparameter tuning is the process which helps select the optimal model architecture. For instance, one such question is: how many trees should I include in my random forest? We used two tuning methods, grid search and random search. Grid search builds a model for all combinations of provided hyperparameter values, and selects the model with the best results through cross validation. Cross validation checks the performance of each combination by analyzing partitions of data and averaging the overall error estimate. Random search defines distributions for the hyperparameters, and not all values are tested - values tested are selected at random. 5.2.3 Training Random Forest Model Advance to the section titled Train Random Forest Model. The first section creates the functions for training the model. It splits the dataset into training and test sets and then fits the model. The second function plots the confusion matrix displaying the model’s accuracy. def split_fit_basic_report(df): &quot;&quot;&quot; Split dataset into training and test, fit model using default RCF parameters return model, prediction, feature_importances, X_train, X_test, y_train, y_test &quot;&quot;&quot; X = df.drop(&#39;class&#39;,axis=1) y = df[&#39;class&#39;] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1, stratify=y) rfc = RandomForestClassifier(random_state = 1) rfc.fit(X_train, y_train) # predict on test data rfc_pred = rfc.predict(X_test) print(classification_report(y_test,rfc_pred)) feature_val = pd.DataFrame(rfc.feature_importances_, index = X_train.columns) print(feature_val.sort_values(0, ascending=False)) return rfc, rfc_pred,feature_val, X_train, X_test, y_train, y_test def plot_cm(y_test, rfc_pred): &quot;&quot;&quot; plot confusion matrix of model predictions &quot;&quot;&quot; p,ax = plt.subplots(figsize=(30, 20)) ConfusionMatrixDisplay.from_predictions(y_test, rfc_pred, ax=ax, colorbar=True) plt.show() Run the subsequent code chunks to use grid search CV and random search CV to select the best estimators for the model. After this is done, we then test the model on unseen data which is not used for training/testing. 5.2.4 Generating Reclassified Images The last section in the workbook is how we visualize the results of our model. We use the model to generate a reclassified image containing pixel predictions for the region of interest. The first section is a dictionary mapping each attribute to the color it will appear in the image, which can be tweaked based on personal preference. li = [&#39;x&#39;, &#39;y&#39;, &#39;R&#39;, &#39;G&#39;, &#39;B&#39;, &#39;temperature&#39;, &#39;year&#39;, &#39;week&#39;, &#39;month&#39;, &#39;season_autumn&#39;, &#39;season_spring&#39;, &#39;season_winter&#39;] def display_predict_img(file): img= np.asarray(Image.open(&quot;invert_&quot;+file)) img_cp = img.copy() ind =np.where((img[:,:,0]!=0) &amp; (img[:,:,1]!=0) &amp; (img[:,:,2]!=0)) y_vals = ind[0] x_vals = ind[1] _= img[y_vals,x_vals,:] r_vals, g_vals, b_vals = _[:,0], _[:,1], _[:,2] fl = merged[merged.orig_name==file][li].head(1) point = pd.DataFrame({&#39;x&#39;: x_vals, &#39;y&#39;: y_vals, &#39;R&#39;: r_vals, &#39;G&#39;: g_vals, &#39;B&#39;: b_vals, }) point[&#39;temperature&#39;]= fl.temperature.values[0] point[&#39;year&#39;]= fl.year.values[0] point[&#39;week&#39;]= fl.week.values[0] point[&#39;month&#39;]= fl.month.values[0] point[&#39;season_autumn&#39;]= fl.season_autumn.values[0] point[&#39;season_spring&#39;]= fl.season_spring.values[0] point[&#39;season_winter&#39;]= fl.season_winter.values[0] pr = model.predict(point) point[&quot;pred_class&quot;] = pr def apply_change(row): pred = row[&quot;pred_class&quot;] newR, newG, newB=colors[class_rgb[pred]] img_cp[row.y,row.x,:] = [newR, newG,newB] point.apply(lambda x: apply_change(x), axis=1) f, ax = plt.subplots(1,2) ax[0].imshow(img) ax[1].imshow(img_cp) f.set_figheight(15) f.set_figwidth(15) return img, img_cp, point This function uses the model output to re-map the image. Below are some outputs generated by the model. The first image shows the model does an excellent job distinguishing between snow and water, practically mapping out the original image exactly. The second image is a bit more ambiguous with a mixture of submerged leaves, ice, and rock, but the model is still able to predict them fairly accurately. 5.3 Future Improvements While our model achieved a high accuracy and recall of 96% for our images, there are still areas for fine-tuning and improvement. For one, the model was occasionally less accurate when attempting to differentiate between water and leaves/rocks in the stream channel, as well as other classes less prevalent in the training set. This is the case in the image here: Another place our model breaks down is wrongly classifying white riffles in water as ice, or wrongly classifying an unedited tube as seen here as ice. Thus, in the future, exposing the model to more training data with these classes can improve its accuracy. Additionally, the model can be expanded to differentiate between different types of ice, which can be of use to researchers studying ice jams - this is just one of many potential changes. "]]
